{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.5.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 6.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (2020.11.13)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (4.57.0)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (3.7.0)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.2-cp36-cp36m-manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 35.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 51.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (1.26.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Installing collected packages: tokenizers, sacremoses, transformers\n",
      "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# process\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "# ml\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from joblib import dump, load\n",
    "import gc \n",
    "import time\n",
    "import pickle\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "KMEANS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ICD Block Names</th>\n",
       "      <th>Title</th>\n",
       "      <th>Research Summary</th>\n",
       "      <th>Inclusion Criteria</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abnormal findings on diagnostic imaging and in...</td>\n",
       "      <td>Checkpoint inhibitor-induced liver injury (ChI...</td>\n",
       "      <td>â¢ Immune checkpoint inhibitors are proven ca...</td>\n",
       "      <td>Both patient groups and control group:  Aged 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abnormal findings on examination of blood, wit...</td>\n",
       "      <td>Exploring the patient experience of a diagnosi...</td>\n",
       "      <td>Research Question: Exploring the patient exper...</td>\n",
       "      <td>â Coded diagnosis of pre-diabetes and have b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abnormal findings on examination of blood, wit...</td>\n",
       "      <td>Assessment of the Impact of a Personalised Nut...</td>\n",
       "      <td>This study will determine if DNA-based dietary...</td>\n",
       "      <td>In order to be eligible to participate in this...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     ICD Block Names  \\\n",
       "0  Abnormal findings on diagnostic imaging and in...   \n",
       "1  Abnormal findings on examination of blood, wit...   \n",
       "2  Abnormal findings on examination of blood, wit...   \n",
       "\n",
       "                                               Title  \\\n",
       "0  Checkpoint inhibitor-induced liver injury (ChI...   \n",
       "1  Exploring the patient experience of a diagnosi...   \n",
       "2  Assessment of the Impact of a Personalised Nut...   \n",
       "\n",
       "                                    Research Summary  \\\n",
       "0  â¢ Immune checkpoint inhibitors are proven ca...   \n",
       "1  Research Question: Exploring the patient exper...   \n",
       "2  This study will determine if DNA-based dietary...   \n",
       "\n",
       "                                  Inclusion Criteria  \n",
       "0  Both patient groups and control group:  Aged 1...  \n",
       "1  â Coded diagnosis of pre-diabetes and have b...  \n",
       "2  In order to be eligible to participate in this...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('final_data.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = df[\"ICD Block Names\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_keep = []\n",
    "for i in true_labels.value_counts().index:\n",
    "    if true_labels.value_counts()[i] >= 0 : \n",
    "        labels_to_keep.append(i)\n",
    "labels_to_keep = list(set(labels_to_keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"ICD Block Names\"].isin(labels_to_keep)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique classes: 224\n"
     ]
    }
   ],
   "source": [
    "true_labels = df[\"ICD Block Names\"] \n",
    "unique_labels = list(set(true_labels.values))\n",
    "useful_labels = [unique_labels.index(label) for label in true_labels]\n",
    "print(\"Number of unique classes:\", len(unique_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = df[[\"Title\",\"Research Summary\",\"Inclusion Criteria\"]].values.T.astype(str)\n",
    "title_content, abstract_content, inclusion_content = content[0], content[1], content[2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12263, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(inclusion_content)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "inclusion_tfidf = pd.DataFrame(denselist, columns=feature_names)\n",
    "\n",
    "pca = PCA(n_components=128)\n",
    "inclusion_tfidf_low_dims  = pca.fit_transform(inclusion_tfidf)\n",
    "del inclusion_tfidf\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vectorizer, open('tfidf_inclusion.sav', 'wb'))\n",
    "pickle.dump(pca, open('pca_inclusion.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(title_content)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "title_tfidf = pd.DataFrame(denselist, columns=feature_names)\n",
    "title_tfidf\n",
    "\n",
    "pca = PCA(n_components=128)\n",
    "title_tfidf_low_dims  = pca.fit_transform(title_tfidf)\n",
    "del title_tfidf\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vectorizer, open('tfidf_title.sav', 'wb'))\n",
    "pickle.dump(pca, open('pca_title.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(abstract_content)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "abstract_tfidf = pd.DataFrame(denselist, columns=feature_names)\n",
    "\n",
    "pca = PCA(n_components=128)\n",
    "abstract_low_dims  = pca.fit_transform(abstract_tfidf)\n",
    "del abstract_tfidf\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vectorizer, open('tfidf_abstract.sav', 'wb'))\n",
    "pickle.dump(pca, open('pca_abstract.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce number of dimensions\n",
    "pca       = PCA(n_components=128)\n",
    "final_rep = np.concatenate([title_tfidf_low_dims,inclusion_tfidf_low_dims,abstract_low_dims], axis=-1)\n",
    "low_dims  = pca.fit_transform(final_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(pca, open('pca_tfidf.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build sample classifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "clf_algo = partial(SVC,C=100 ,kernel=\"rbf\", probability=True, class_weight=\"balanced\")\n",
    "# partial(KNeighborsClassifier, metric=\"cosine\") # BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "import warnings \n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_proportion = 0.15\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training data is:  10530\n",
      "Size of validation data is:  1733\n"
     ]
    }
   ],
   "source": [
    "np_labels = np.array(useful_labels)\n",
    "idxs = {c: np.argwhere(np_labels == c).ravel() for c in np.unique(np_labels)}\n",
    "idxs_set = {c: {\"train\": [], \"val\": []} for c in np.unique(np_labels)}\n",
    "\n",
    "# random seed so everytime the split is the same\n",
    "for c in np.unique(np_labels):\n",
    "    amount = int(len(idxs[c]) * val_proportion)\n",
    "    choices = np.random.choice(idxs[c], size=amount, replace=False)\n",
    "    # always check that not all idxs are destined to validation\n",
    "    if len(choices) < len(idxs[c]):\n",
    "        # assign the choices to validation and rest to training\n",
    "        idxs_set[c][\"val\"]   = choices.tolist()\n",
    "        idxs_set[c][\"train\"] = [i for i in idxs[c] if i not in idxs_set[c][\"val\"]]\n",
    "\n",
    "# merge indexes for all classes\n",
    "train_idxs, val_idxs = [], []\n",
    "for c in np.unique(np_labels):\n",
    "    val_idxs.extend( idxs_set[c][\"val\"] ) \n",
    "    train_idxs.extend( idxs_set[c][\"train\"] ) \n",
    "    \n",
    "# get data splitted accordingly:\n",
    "x_train, y_train = np.array(low_dims)[train_idxs], np.array(useful_labels)[train_idxs]\n",
    "x_val, y_val     = np.array(low_dims)[val_idxs], np.array(useful_labels)[val_idxs]\n",
    "\n",
    "# finally, check sizes:\n",
    "print(\"Size of training data is: \", len(x_train))\n",
    "print(\"Size of validation data is: \", len(x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training_time : 112.02108383178711\n",
      "A more realistic estimate on training data can be: 0.798005698005698 in top 1\n",
      "A more realistic estimate on training data can be: 0.9696106362773029 in top 3 \n",
      "\n",
      "A more realistic estimate on validation data can be: 0.6664743219849971 in top 1\n",
      "A more realistic estimate on validation data can be: 0.9226774379688402 in top 3\n",
      "177.20954990386963\n"
     ]
    }
   ],
   "source": [
    "# get and estimate of how well it'd do on unseen data\n",
    "clf = clf_algo()\n",
    "#clf = LGBMClassifier()\n",
    "#clf = xgb.XGBClassifier()\n",
    "#clf = OneVsRestClassifier(xgb.XGBClassifier(tree_method='gpu_hist'))\n",
    "#clf = OneVsRestClassifier(clf_algo())\n",
    "begin_train = time.time()\n",
    "clf.fit(x_train, y_train)\n",
    "end_train = time.time()\n",
    "print('Training_time :',end_train-begin_train)\n",
    "\n",
    "begin = time.time()\n",
    "TOP_K = 3\n",
    "# for training data\n",
    "pred_probs_train  = clf.predict_proba(x_train)\n",
    "in_or_out_train   = []\n",
    "top_k_preds_train = []\n",
    "for i,pred in enumerate(pred_probs_train):\n",
    "    reordered = np.zeros(len(unique_labels))\n",
    "    reordered[clf.classes_]  = pred\n",
    "    this_top_k = np.argsort(reordered)[::-1][:TOP_K] \n",
    "    top_k_preds_train.append(this_top_k)\n",
    "    in_or_out_train.append( y_train[i] in this_top_k )\n",
    "\n",
    "\n",
    "# for validation data\n",
    "pred_probs_val  = clf.predict_proba(x_val)\n",
    "in_or_out_val   = []\n",
    "top_k_preds_val = []\n",
    "for i,pred in enumerate(pred_probs_val):\n",
    "    reordered = np.zeros(len(unique_labels))\n",
    "    reordered[clf.classes_]  = pred\n",
    "    this_top_k = np.argsort(reordered)[::-1][:TOP_K] \n",
    "    top_k_preds_val.append(this_top_k)\n",
    "    in_or_out_val.append( y_val[i] in this_top_k )\n",
    "    \n",
    "# print results\n",
    "print(\"A more realistic estimate on training data can be:\", np.mean(clf.predict(x_train) == y_train), \"in top 1\")\n",
    "print(\"A more realistic estimate on training data can be:\", np.mean(in_or_out_train), \"in top\", TOP_K, \"\\n\")\n",
    "print(\"A more realistic estimate on validation data can be:\", np.mean(clf.predict(x_val) == y_val), \"in top 1\")\n",
    "print(\"A more realistic estimate on validation data can be:\", np.mean(in_or_out_val), \"in top\", TOP_K)\n",
    "\n",
    "# write to file\n",
    "is_train = []\n",
    "top_1, top_2, top_3 = [], [], []\n",
    "for i in range(len(useful_labels)):\n",
    "    if i in val_idxs:\n",
    "        is_train.append(\"val\")\n",
    "        top_1.append( unique_labels[top_k_preds_val[ val_idxs.index(i) ][0]] )\n",
    "        top_2.append( unique_labels[top_k_preds_val[ val_idxs.index(i) ][1]] )\n",
    "        top_3.append( unique_labels[top_k_preds_val[ val_idxs.index(i) ][2]] )\n",
    "    else:\n",
    "        is_train.append(\"train\")\n",
    "        top_1.append( unique_labels[top_k_preds_train[ train_idxs.index(i) ][0]] )\n",
    "        top_2.append( unique_labels[top_k_preds_train[ train_idxs.index(i) ][1]] )\n",
    "        top_3.append( unique_labels[top_k_preds_train[ train_idxs.index(i) ][2]] )\n",
    "        \n",
    "df[\"first_guess\"]  = top_1\n",
    "df[\"second_guess\"] = top_2\n",
    "df[\"third_guess\"]  = top_3\n",
    "df[\"is_training\"] = is_train\n",
    "df.to_csv(\"with_predictions.csv\")\n",
    "end = time.time()\n",
    "print(end - begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(clf, open('clf_tfidf.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
